# Default values for spark chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# meta:
#   owner:
#   gitrepo:

replicaCount: 1

project:
  # default service/component name, free to change
  component: gdt
  namespace: gdt

applicationType: workflow

appLabels:
  # custom:
  #   key: value
  custom: {}

runAsJob: false

image:
  repository: rehanann/spark-py
  tag: 3.4.0-extended
  pullPolicy: Always
  command: ["/bin/bash"]
  args: []
#  command: ["/opt/spark/bin/spark-submit"]
# environment variables
envVars:
  driver:
    SPARK_LOG_DIR: /opt/spark/work-dir
  executor:
    SPARK_LOG_DIR: /opt/spark/work-dir

# This allow to add any required key values to spark configuration, unless it should not duplicated.
# e.g.
# sparkconf:
#  spark.sql.maxPlanStringLength: "2147483632"
sparkConf:
  # spark.eventLog.enabled: true
  # spark.eventLog.dir: s3a://historyservice/logs
  spark.hadoop.fs.s3a.endpoint: http://minio.default.svc.cluster.local:9000
  spark.hadoop.fs.s3a.access.key: bU0c4naTz98KynxEsHf1
  spark.hadoop.fs.s3a.secret.key: dNOkRi7cbUsw0ZHnN9coosKO0iyu21XJpieAayZd
  spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
  spark.hadoop.fs.s3a.path.style.access: true
  spark.hadoop.fs.s3a.connection.ssl.enabled: false
  spark.hadoop.fs.s3n.impl: org.apache.hadoop.fs.s3native.NativeS3FileSystem
  spark.hadoop.fs.s3a.experimental.fadvise: random
  spark.sql.sources.partitionOverwriteMode: static
  spark.sql.sources.partitionColumnTypeInference.enabled: false
  spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: 2
  spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled: true

pyspark:
  python: /usr/bin/python3
  requirements:
    #Fixed configmap file path location to /tmp/requirements.txt, this removes this PVC mandatory requirements.
    filePath: /tmp/pip3_requirements.txt

sparkClientPod:
  required: true

services:
  ui:
    enabled: true
    type: ClusterIP
    port: 4040
  driver:
    enabled: true
    type: ClusterIP
    port: 3000

driver:
  cores: 1
  memory: 1g
  limit:
    cores: 1
  request:
    cores: 1
executor:
  cores: 1
  memory: 1g
  memoryOverhead: 1g
  request:
    cores: 1
  limit:
    cores: 1

resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 1
    memory: 1Gi

dynamicAllocation:
  enabled: false
  executorIdleTimeout: 30s
  # Values are chosen based on current YARN setting and existing quotas
  executors:
    minExecutors: 1
    initialExecutors: 1
    maxExecutors: 2
  shuffle:
    # disable external shuffle service, in k8s it is not used
    service:
      enabled: false
    # instead of external shuffle service, tracking of pods which have shuffle blocks is used
    tracking:
      enabled: false
    # timeout for the the pod to keep shuffle blocks
    timeout: 30s

# dependencies: {}

sparkLogs: ERROR

# Below settings control spark 3.5 (log4j2) log level values can be: error, warn, info, debug.
spark35LogLevel: error

sharedVolume:
  enabled: true
  useExisting: true
  mountPath: /opt/spark/work-dir/shared
  existingPvcName: gdt-shared

eventLogs:
  # spark.eventLog.enabled
  enabled: true
  # spark.eventLog.dir
  #  Example for shared PVC
  #  directory: /opt/spark/work-dir/shared/history
  # directory: /tmp/spark-events
  directory: s3a://historyservice/logs
